[
  {
    "objectID": "posts/2022-03-15-shared-data.html",
    "href": "posts/2022-03-15-shared-data.html",
    "title": "Sharing matrix data between OpenFrameworks and Julia",
    "section": "",
    "text": "My current project incorporates a swarming simulation written in C++ using the OpenFrameworks toolkit. All my machine learning research has been written in Julia. Since I require real-time analysis of the swarm data, I need to send data from my simulation into julia for processing. There are a number of ways to go about this:\n\nNetwork based data exchange options:\n\nSomething like Open Sound Control which is commonly used for musical data exchange although not typically intended for large quantities of data. Also it is UDP based, so depending on network configuration/stability data could be lost.\nA custom data exchange protocol - quite a lot of implementation/testing testing time involved.\nAny network based approach would require serialization/de-serialization which is likely to make it prohibitively costly given the quantity of data we are attempting to transmit.\n\nA shared data file\n\nWould require the simulation to write to a file, while julia reads from the file. Synchronization mechanism would be needed. Seems wasteful.\n\nIPC (Inter Process Communication) e.g. Shared memory\n\nIf the raw data from the simulation could be shared with the Julia process via a shared memory address then it would be a very efficient, since no serialization, disk IO, or network IO, would be required. Synchronization would still need to be managed to avoid threading issues."
  },
  {
    "objectID": "posts/2022-03-15-shared-data.html#posix-shared-memory",
    "href": "posts/2022-03-15-shared-data.html#posix-shared-memory",
    "title": "Sharing matrix data between OpenFrameworks and Julia",
    "section": "POSIX shared memory",
    "text": "POSIX shared memory\nThe primary function for creating shared memory is via the shm_open call:\nint shm_open(const char* name, int flags,  mode_t mode);\nThis creates a new (or opens and existing) shared memory object and returns a file descriptor. In the case of of it creating a new shared memory object, it will have a default size of 0 bytes. The call ftruncate can be used to set the appropriate size of the shared memory object:\nint ftruncate(int file_descriptor, off_t memory_size);\nThis simply returns 0 on success and -1 otherwise.\nOnce the shared memory object has been setup and initialized then the data needs to be mapped into this address space using the call mmap\nvoid* mmap(void *addr, size_t memory_size, int prot, int flags, int fd, off_t offset);\nIn our case the address parameter is simply set to NULL since we don’t care where the kernel uses memory from. The prot argument describes the desired memory protection of the mapping, whether it can be read, written executed. The flags argument specifies (among other things) if this mapping is shared across processes, or is private. In our case we set it to MAP_SHARED. fd is the file descriptor of the shared memory object, and offset is unused in our case.\nThere are also the calls munmap and shm_unlink used to clean up allocated resources."
  },
  {
    "objectID": "posts/2022-03-15-shared-data.html#posix-mutex",
    "href": "posts/2022-03-15-shared-data.html#posix-mutex",
    "title": "Sharing matrix data between OpenFrameworks and Julia",
    "section": "POSIX Mutex",
    "text": "POSIX Mutex\nSince the simulation will be writing to the shared memory and the julia process will be reading concurrently, we still need to synchronise their access to the shared memory object. For this the POSIX thread library provides an implementation of a mutex.\nFor a general overview I found this website helpful: https://riptutorial.com/posix/example/15910/simple-mutex-usage\nOne additional implementation detail is that we’re going to use shared memory again to store this mutex, meaning that both the simulation and the julia process can use it. This means more calls to shm_open, ftruncate and mmap.\nThis is in addition to the usual POSIX mutex initialisation, including:\n\npthread_mutex_init\npthread_mutexattr_setpshared"
  },
  {
    "objectID": "posts/2022-02-06-dcae_part_2.html",
    "href": "posts/2022-02-06-dcae_part_2.html",
    "title": "Deep Convolutional Auto Encoders - Part 2",
    "section": "",
    "text": "In the first part I reimplemented the convolutional auto encoder from TimeCluster by Ali et al This time, I will adapt the model to handle all 300 flock agents.\nIn this notebook a 1D convolutional approach is evaluated"
  },
  {
    "objectID": "posts/2022-02-06-dcae_part_2.html#data-noramlisation-and-preperation",
    "href": "posts/2022-02-06-dcae_part_2.html#data-noramlisation-and-preperation",
    "title": "Deep Convolutional Auto Encoders - Part 2",
    "section": "Data Noramlisation and preperation",
    "text": "Data Noramlisation and preperation\nNormalise the data the same way as before; into the range [0, 1] as per the paper.\nWe then create a sliding window using the defaults from the paper where stride = 1 and window_size = 60\nThen we shuffle the data and split into train, validate and test subsets\n\nfunction normalise(M) \n    min = minimum(minimum(eachcol(M)))\n    max = maximum(maximum(eachcol(M)))\n    return (M .- min) ./ (max - min)\nend\n\nnormalised = Array(df) |> normalise\n\nwindow_size = 60\n\ndata = slidingwindow(normalised',window_size,stride=1)\n\ntrain, validate, test = splitobs(shuffleobs(data), (0.7,0.2));"
  },
  {
    "objectID": "posts/2022-02-06-dcae_part_2.html#define-the-encoder-and-decoder",
    "href": "posts/2022-02-06-dcae_part_2.html#define-the-encoder-and-decoder",
    "title": "Deep Convolutional Auto Encoders - Part 2",
    "section": "Define the encoder and decoder",
    "text": "Define the encoder and decoder\nWe can define the network shape in a couple of different ways:\n\nKeeping the convolution 1 dimentional and simply increasing the number of features from 3 to 900 (3 * num_of_agents)\nUsing 2D convolution: window_size X num_of_agents x dimensions (3) x batch\n\nIn this notebook we will look at the 1D approach."
  },
  {
    "objectID": "posts/2022-02-06-dcae_part_2.html#d-convolution",
    "href": "posts/2022-02-06-dcae_part_2.html#d-convolution",
    "title": "Deep Convolutional Auto Encoders - Part 2",
    "section": "1D Convolution",
    "text": "1D Convolution\nAdjusted the dimension expansion from ≈ 21x to 10x Also an aditional Conv/ConvTranspose step is added to reduce the dimensionality of the encoded space further\n\n\nfunction create_ae_1d()\n  # Define the encoder and decoder networks \n  encoder = Chain(\n  # 60x900xb\n  Conv((9,), 900 => 9000, relu; pad = SamePad()),\n  MaxPool((2,)),\n  # 30x9000xb\n  Conv((5,), 9000 => 4500, relu; pad = SamePad()),\n  MaxPool((2,)),\n  # 15x4500xb\n  Conv((5,),4500 => 2250, relu; pad = SamePad()),\n  # 15x2250xb\n  MaxPool((3,)),\n  Conv((3,),2250 => 1000, relu; pad = SamePad()),\n  Conv((3,),1000 => 100, relu; pad = SamePad()),\n  # 5x100xb\n  Flux.flatten,\n  Dense(500,100)\n)\ndecoder = Chain(\n  Dense(100,500),\n  (x -> reshape(x, 5,100,:)),\n  # 5x100xb\n  ConvTranspose((3,), 100  => 1000, relu; pad = SamePad()),\n  ConvTranspose((3,), 1000 => 2250, relu; pad = SamePad()),\n  Upsample((3,)),\n  # 15x2250xb\n  ConvTranspose((5,), 2250 => 4500, relu; pad = SamePad()),\n  Upsample((2,)),\n  # 30x4500xb\n  ConvTranspose((5,), 4500 => 9000, relu; pad = SamePad()),\n  Upsample((2,)),\n  # 60x9000xb\n  ConvTranspose((9,), 9000 => 900, relu; pad = SamePad()),\n  # 60x900xb\n)\nreturn (encoder, decoder)\nend\n\ncreate_ae_1d (generic function with 1 method)"
  },
  {
    "objectID": "posts/2022-02-06-dcae_part_2.html#training",
    "href": "posts/2022-02-06-dcae_part_2.html#training",
    "title": "Deep Convolutional Auto Encoders - Part 2",
    "section": "Training",
    "text": "Training\nTraining needs to be slightly adapted for each version of model we use. Also we now use train/validation/test sets for more accurate performance calculation. I’ve also added learning rate adjustment and automatic model saving.\n\nfunction save_model(m, epoch, loss)\n    model_row = LegolasFlux.ModelRow(; weights = fetch_weights(cpu(m)),architecture_version=1, loss=0.0001)\n    write_model_row(\"1d_300_model-$epoch-$loss.arrow\", model_row)\nend\n\nfunction rearrange_1D(x)\n    permutedims(cat(x..., dims=3), [2,1,3])\nend\n\nfunction train_model_1D!(model, train, validate, opt; epochs=20, bs=16, dev=Flux.gpu)\n    ps = Flux.params(model)\n    local train_loss, train_loss_acc\n    local validate_loss, validate_loss_acc\n    local last_improvement = 0\n    local prev_best_loss = 0.01\n    local improvement_thresh = 5.0\n    validate_losses = Vector{Float64}()\n    for e in 1:epochs\n        train_loss_acc = 0.0\n        for x in eachbatch(train, size=bs)\n            x  = rearrange_1D(x) |> dev\n            gs = Flux.gradient(ps) do\n                train_loss = Flux.Losses.mse(model(x),x)\n                return train_loss\n            end\n            train_loss_acc += train_loss\n            Flux.update!(opt, ps, gs)\n        end\n        validate_loss_acc = 0.0\n        for y in eachbatch(validate, size=bs)\n            y  = rearrange_1D(y) |> dev\n            validate_loss = Flux.Losses.mse(model(y), y)\n            validate_loss_acc += validate_loss\n        end\n        validate_loss_acc = round(validate_loss_acc / (length(validate)/bs); digits=6)\n        train_loss_acc = round(train_loss_acc / (length(train)/bs) ;digits=6)\n        if validate_loss_acc < 0.001\n            if validate_loss_acc < prev_best_loss\n                @info \"new best accuracy $validate_loss_acc saving model...\"\n                save_model(model, e, validate_loss_acc)\n                last_improvement = e\n                prev_best_loss = validate_loss_acc\n            elseif (e - last_improvement) >= improvement_thresh && opt.eta > 1e-5\n                @info \"Not improved in $improvement_thresh epochs. Dropping learning rate to $(opt.eta / 2.0)\"\n                opt.eta /= 2.0\n                last_improvement = e # give it some time to improve\n                improvement_thresh = improvement_thresh * 1.5\n            elseif (e - last_improvement) >= 15\n                @info \"Not improved in 15 epochs. Converged I guess\"\n                break\n            end\n        end\n        push!(validate_losses, validate_loss_acc)\n        println(\"Epoch $e/$epochs\\t train loss: $train_loss_acc\\t validate loss: $validate_loss_acc\")\n    end\n    validate_losses\n end\n    \n\ntrain_model_1D! (generic function with 1 method)\n\n\n\n#collapse_output\nlosses_0001       = train_model_1D!(model, train, validate, Flux.Optimise.ADAM(0.0001); epochs=200, bs=48);\n\n┌ Warning: The specified values for size and/or count will result in 21 unused data points\n└ @ MLDataPattern /opt/julia/packages/MLDataPattern/KlSmO/src/dataview.jl:205\n\n\nEpoch 1/200  train loss: 0.430836    validate loss: 0.057712\nEpoch 2/200  train loss: 0.055009    validate loss: 0.051833\nEpoch 3/200  train loss: 0.053515    validate loss: 0.051719\nEpoch 4/200  train loss: 0.053483    validate loss: 0.051669\nEpoch 5/200  train loss: 0.053481    validate loss: 0.051648\nEpoch 6/200  train loss: 0.05348     validate loss: 0.051622\nEpoch 7/200  train loss: 0.053478    validate loss: 0.051622\nEpoch 8/200  train loss: 0.053471    validate loss: 0.051615\nEpoch 9/200  train loss: 0.053464    validate loss: 0.051602\nEpoch 10/200     train loss: 0.053461    validate loss: 0.051603\nEpoch 11/200     train loss: 0.053447    validate loss: 0.051597\nEpoch 12/200     train loss: 0.053427    validate loss: 0.051574\nEpoch 13/200     train loss: 0.053222    validate loss: 0.050061\nEpoch 14/200     train loss: 0.040074    validate loss: 0.033838\nEpoch 15/200     train loss: 0.034933    validate loss: 0.033222\nEpoch 16/200     train loss: 0.029958    validate loss: 0.018663\nEpoch 17/200     train loss: 0.015721    validate loss: 0.012654\nEpoch 18/200     train loss: 0.011821    validate loss: 0.010214\nEpoch 19/200     train loss: 0.009049    validate loss: 0.007028\nEpoch 20/200     train loss: 0.006481    validate loss: 0.005952\nEpoch 21/200     train loss: 0.004894    validate loss: 0.003345\nEpoch 22/200     train loss: 0.002088    validate loss: 0.001259\nEpoch 23/200     train loss: 0.001   validate loss: 0.001243\n\n\n┌ Info: new best accuracy 0.000762 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 24/200     train loss: 0.000691    validate loss: 0.000762\n\n\n┌ Info: new best accuracy 0.000597 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 25/200     train loss: 0.000683    validate loss: 0.000597\n\n\n┌ Info: new best accuracy 0.000309 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 26/200     train loss: 0.000584    validate loss: 0.000309\n\n\n┌ Info: new best accuracy 0.000142 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 27/200     train loss: 0.000244    validate loss: 0.000142\n\n\n┌ Info: new best accuracy 9.1e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 28/200     train loss: 0.000118    validate loss: 9.1e-5\n\n\n┌ Info: new best accuracy 7.4e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 29/200     train loss: 8.6e-5  validate loss: 7.4e-5\n\n\n┌ Info: new best accuracy 6.3e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 30/200     train loss: 7.1e-5  validate loss: 6.3e-5\nEpoch 31/200     train loss: 6.5e-5  validate loss: 6.5e-5\n\n\n┌ Info: new best accuracy 5.6e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 32/200     train loss: 6.3e-5  validate loss: 5.6e-5\n\n\n┌ Info: new best accuracy 4.8e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 33/200     train loss: 5.3e-5  validate loss: 4.8e-5\n\n\n┌ Info: new best accuracy 4.2e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 34/200     train loss: 4.6e-5  validate loss: 4.2e-5\n\n\n┌ Info: new best accuracy 3.9e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 35/200     train loss: 4.2e-5  validate loss: 3.9e-5\n\n\n┌ Info: new best accuracy 3.6e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 36/200     train loss: 3.8e-5  validate loss: 3.6e-5\n\n\n┌ Info: new best accuracy 3.3e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 37/200     train loss: 3.5e-5  validate loss: 3.3e-5\n\n\n┌ Info: new best accuracy 3.1e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 38/200     train loss: 3.3e-5  validate loss: 3.1e-5\n\n\n┌ Info: new best accuracy 2.9e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 39/200     train loss: 3.1e-5  validate loss: 2.9e-5\n\n\n┌ Info: new best accuracy 2.7e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 40/200     train loss: 2.9e-5  validate loss: 2.7e-5\nEpoch 41/200     train loss: 2.7e-5  validate loss: 2.8e-5\n\n\n┌ Info: new best accuracy 2.6e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 42/200     train loss: 2.7e-5  validate loss: 2.6e-5\nEpoch 43/200     train loss: 3.0e-5  validate loss: 2.9e-5\nEpoch 44/200     train loss: 6.2e-5  validate loss: 8.7e-5\nEpoch 45/200     train loss: 0.000182    validate loss: 0.000175\nEpoch 46/200     train loss: 0.000311    validate loss: 0.000224\nEpoch 47/200     train loss: 0.00104     validate loss: 0.000602\n\n\n┌ Info: Not improved in 5 epochs. Dropping learning rate to 5.0e-5\n└ @ Main In[9]:46\n\n\nEpoch 48/200     train loss: 0.000221    validate loss: 5.7e-5\nEpoch 49/200     train loss: 3.7e-5  validate loss: 2.6e-5\n\n\n┌ Info: new best accuracy 2.4e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 50/200     train loss: 2.5e-5  validate loss: 2.4e-5\n\n\n┌ Info: new best accuracy 2.2e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 51/200     train loss: 2.2e-5  validate loss: 2.2e-5\n\n\n┌ Info: new best accuracy 2.1e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 52/200     train loss: 2.1e-5  validate loss: 2.1e-5\n\n\n┌ Info: new best accuracy 2.0e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 53/200     train loss: 2.0e-5  validate loss: 2.0e-5\n\n\n┌ Info: new best accuracy 1.9e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 54/200     train loss: 1.9e-5  validate loss: 1.9e-5\n\n\n┌ Info: new best accuracy 1.8e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 55/200     train loss: 1.9e-5  validate loss: 1.8e-5\nEpoch 56/200     train loss: 1.8e-5  validate loss: 1.8e-5\n\n\n┌ Info: new best accuracy 1.7e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 57/200     train loss: 1.8e-5  validate loss: 1.7e-5\nEpoch 58/200     train loss: 1.7e-5  validate loss: 1.7e-5\n\n\n┌ Info: new best accuracy 1.6e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 59/200     train loss: 1.7e-5  validate loss: 1.6e-5\nEpoch 60/200     train loss: 1.7e-5  validate loss: 1.6e-5\nEpoch 61/200     train loss: 1.6e-5  validate loss: 1.6e-5\nEpoch 62/200     train loss: 1.6e-5  validate loss: 1.6e-5\n\n\n┌ Info: new best accuracy 1.5e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 63/200     train loss: 1.6e-5  validate loss: 1.5e-5\nEpoch 64/200     train loss: 1.5e-5  validate loss: 1.5e-5\nEpoch 65/200     train loss: 1.5e-5  validate loss: 1.5e-5\nEpoch 66/200     train loss: 1.5e-5  validate loss: 1.5e-5\n\n\n┌ Info: new best accuracy 1.4e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 67/200     train loss: 1.5e-5  validate loss: 1.4e-5\nEpoch 68/200     train loss: 1.4e-5  validate loss: 1.4e-5\nEpoch 69/200     train loss: 1.4e-5  validate loss: 1.4e-5\nEpoch 70/200     train loss: 1.4e-5  validate loss: 1.4e-5\n\n\n┌ Info: new best accuracy 1.3e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 71/200     train loss: 1.4e-5  validate loss: 1.3e-5\nEpoch 72/200     train loss: 1.4e-5  validate loss: 1.3e-5\nEpoch 73/200     train loss: 1.3e-5  validate loss: 1.3e-5\nEpoch 74/200     train loss: 1.3e-5  validate loss: 1.3e-5\nEpoch 75/200     train loss: 1.3e-5  validate loss: 1.3e-5\n\n\n┌ Info: new best accuracy 1.2e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 76/200     train loss: 1.3e-5  validate loss: 1.2e-5\nEpoch 77/200     train loss: 1.2e-5  validate loss: 1.2e-5\nEpoch 78/200     train loss: 1.2e-5  validate loss: 1.2e-5\nEpoch 79/200     train loss: 1.2e-5  validate loss: 1.2e-5\nEpoch 80/200     train loss: 1.2e-5  validate loss: 1.2e-5\n\n\n┌ Info: new best accuracy 1.1e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 81/200     train loss: 1.2e-5  validate loss: 1.1e-5\nEpoch 82/200     train loss: 1.1e-5  validate loss: 1.1e-5\nEpoch 83/200     train loss: 1.1e-5  validate loss: 1.1e-5\nEpoch 84/200     train loss: 1.1e-5  validate loss: 1.1e-5\nEpoch 85/200     train loss: 1.1e-5  validate loss: 1.1e-5\nEpoch 86/200     train loss: 1.1e-5  validate loss: 1.1e-5\nEpoch 87/200     train loss: 1.1e-5  validate loss: 1.1e-5\nEpoch 88/200     train loss: 1.1e-5  validate loss: 1.1e-5\nEpoch 89/200     train loss: 1.1e-5  validate loss: 1.1e-5\n\n\n┌ Info: Not improved in 5 epochs. Dropping learning rate to 2.5e-5\n└ @ Main In[9]:46\n┌ Info: new best accuracy 1.0e-5 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 90/200     train loss: 1.0e-5  validate loss: 1.0e-5\nEpoch 91/200     train loss: 1.0e-5  validate loss: 1.0e-5\nEpoch 92/200     train loss: 1.0e-5  validate loss: 1.0e-5\nEpoch 93/200     train loss: 1.0e-5  validate loss: 1.0e-5\nEpoch 94/200     train loss: 1.0e-5  validate loss: 1.0e-5\nEpoch 95/200     train loss: 1.0e-5  validate loss: 1.0e-5\nEpoch 96/200     train loss: 1.0e-5  validate loss: 1.0e-5\n\n\n┌ Info: new best accuracy 9.0e-6 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 97/200     train loss: 1.0e-5  validate loss: 9.0e-6\nEpoch 98/200     train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 99/200     train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 100/200    train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 101/200    train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 102/200    train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 103/200    train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 104/200    train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 105/200    train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 106/200    train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 107/200    train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 108/200    train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 109/200    train loss: 9.0e-6  validate loss: 9.0e-6\n\n\n┌ Info: Not improved in 5 epochs. Dropping learning rate to 1.25e-5\n└ @ Main In[9]:46\n\n\nEpoch 110/200    train loss: 9.0e-6  validate loss: 9.0e-6\nEpoch 111/200    train loss: 9.0e-6  validate loss: 9.0e-6\n\n\n┌ Info: new best accuracy 8.0e-6 saving model...\n└ @ Main In[9]:41\n\n\nEpoch 112/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 113/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 114/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 115/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 116/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 117/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 118/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 119/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 120/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 121/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 122/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 123/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 124/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 125/200    train loss: 8.0e-6  validate loss: 8.0e-6\nEpoch 126/200    train loss: 8.0e-6  validate loss: 8.0e-6\n\n\n┌ Info: Not improved in 10 epochs. Converged I guess\n└ @ Main In[9]:51"
  },
  {
    "objectID": "posts/2022-02-06-dcae_part_2.html#results",
    "href": "posts/2022-02-06-dcae_part_2.html#results",
    "title": "Deep Convolutional Auto Encoders - Part 2",
    "section": "Results",
    "text": "Results\nWe managed to get a validation loss of 8.0e-6, taking a 900x60 space down into a 1x100 vector\nLets see what this means in practical terms by comparing an input to an output:\n\n#lets grab a random window of data and visualise it\ntest_data = rand(test)\ncreate_gif_from_raw(test_data)\n\n┌ Info: Saved animation to \n│   fn = /notebooks/anim_fps30.gif\n└ @ Plots /opt/julia/packages/Plots/LSKOd/src/animation.jl:114\n\n\n\n\n\n\n#now lets push that window through our autoencoder and visualise the reconstruction\ninput = Flux.unsqueeze(test_data', 3)\noutput = new_model(input)\noutput = reshape(output, 60,900)'\ncreate_gif_from_raw(output)\n\n┌ Info: Saved animation to \n│   fn = /notebooks/anim_fps30.gif\n└ @ Plots /opt/julia/packages/Plots/LSKOd/src/animation.jl:114"
  },
  {
    "objectID": "posts/2022-02-06-dcae_part_2.html#conclusion",
    "href": "posts/2022-02-06-dcae_part_2.html#conclusion",
    "title": "Deep Convolutional Auto Encoders - Part 2",
    "section": "Conclusion",
    "text": "Conclusion\nAfter a few hours of training on GPU, we can now reasonably encode the movement of the whole swarm (300 agents) over 60 timesteps into 100 variables. However, I want to reduce that encoding even further into ~10 parameters that can be used to sonify the dynamics.\nNext time I will see how much further a can reduce the latent space - as well as seeing how useful other DR methods are when applied to the latent space."
  },
  {
    "objectID": "posts/2022-02-28-dimension_reduction.html",
    "href": "posts/2022-02-28-dimension_reduction.html",
    "title": "Dimension reduction of the latent space",
    "section": "",
    "text": "In the first part I reimplemented the convolutional auto encoder from TimeCluster by Ali et al. The the second part, I adapted the model to handle all 300 flock agents.\nIn this notebook I evaluate serveral processes to turn the latent space representation into parameters for musical control\n\nfunction normalise(M) \n    min = minimum(minimum(eachcol(M)))\n    max = maximum(maximum(eachcol(M)))\n    return (M .- min) ./ (max - min)\nend\n\nnormalised = Array(df) |> normalise\n\nwindow_size = 60\n\ndata = slidingwindow(normalised',window_size,stride=1)\n\ntrain, validate, test = splitobs(shuffleobs(data), (0.7,0.2));\n\n\nfunction create_ae_1d()\n  # Define the encoder and decoder networks \n  encoder = Chain(\n  # 60x900xb\n  Conv((9,), 900 => 9000, relu; pad = SamePad()),\n  MaxPool((2,)),\n  # 30x9000xb\n  Conv((5,), 9000 => 4500, relu; pad = SamePad()),\n  MaxPool((2,)),\n  # 15x4500xb\n  Conv((5,),4500 => 2250, relu; pad = SamePad()),\n  # 15x2250xb\n  MaxPool((3,)),\n  Conv((3,),2250 => 1000, relu; pad = SamePad()),\n  Conv((3,),1000 => 100, relu; pad = SamePad()),\n  # 5x100xb\n  Flux.flatten,\n  Dense(500,100)\n)\ndecoder = Chain(\n  Dense(100,500),\n  (x -> reshape(x, 5,100,:)),\n  # 5x100xb\n  ConvTranspose((3,), 100  => 1000, relu; pad = SamePad()),\n  ConvTranspose((3,), 1000 => 2250, relu; pad = SamePad()),\n  Upsample((3,)),\n  # 15x2250xb\n  ConvTranspose((5,), 2250 => 4500, relu; pad = SamePad()),\n  Upsample((2,)),\n  # 30x4500xb\n  ConvTranspose((5,), 4500 => 9000, relu; pad = SamePad()),\n  Upsample((2,)),\n  # 60x9000xb\n  ConvTranspose((9,), 9000 => 900, relu; pad = SamePad()),\n  # 60x900xb\n)\nreturn (encoder, decoder)\nend\n\ncreate_ae_1d (generic function with 1 method)\n\n\n\n\nWe load the saved model that we trained in the previous step\n\nencoder, decoder  = create_ae_1d()\nnew_model         = Chain(encoder, decoder)\nmodel_row         = read_model_row(\"1d_300_model-112-8.0e-6.arrow\")\nload_weights!(new_model, model_row.weights)\n\n\n@userplot FlockPlot\n@recipe function f(cp::FlockPlot)\n    x,y,z = cp.args\n    color := :black\n    label --> false\n    seriestype := :scatter\n    alpha := 0.75\n    markersize := 3\n    xlim := (0, 1)\n    ylim := (0, 1)\n    zlim := (0, 1)\n    x, y, z\nend\n\nfunction create_gif_of_data_latent_space_and_reduced(data, encoded, components, title)\n    s = size(components,1)\n    t = reshape(getobs(data), 3, 300, s)\n    t = permutedims(t, [2,3,1])\n    anim = @animate for i ∈ 1:s\n        A = flockplot(t[:,i,1], t[:,i,2], t[:,i,3],  title = \"Input Swarm\")\n        B = Plots.bar(components[i], title=title, label=\"\"; ylim=(-35,35))\n        C = Plots.heatmap(reshape(encoded[i], (10,10)),colorbar=false, color=:autumn, title=\"Encoded Representation\", axis=([], false))\n        Plots.plot(A,C,B;layout=(1,3),size = (1100, 400))\n    end\n    gif(anim, \"anim_fps30.gif\", fps=30)\nend\n    \n\ncreate_gif_of_data_latent_space_and_PCA (generic function with 1 method)\n\n\n\n\n\nNow that we can encode the movement of the whole swarm over 60 timesteps into 100 variables, I want to reduce that encoding even further into ~10 parameters that can be used to sonify the dynamics.\nSince we’ve already used a non linear dimension reduction on our data, lets evaluate PCA, ISOMAP, UMAP to reduce the latent space further.\n\n# Gather all our encoded data into training and test matricies\n\ntrain_encoded = Matrix{Float32}[]\nfor t ∈ train\n    i      = Flux.unsqueeze(t', 3)\n    output = hcat(new_model[1](i)...)\n    push!(train_encoded, output)\nend\n\ntest_encoded = Matrix{Float32}[]\nfor t ∈ test\n    i      = Flux.unsqueeze(t', 3)\n    output = hcat(new_model[1](i)...)\n    push!(test_encoded, output)\nend\n\n\n\n\nM = fit(PCA, vcat(train_encoded...)'; maxoutdim=10)\n\nPCA(indim = 100, outdim = 7, principalratio = 0.9925308)\n\n\n\nvarience_preserved = principalratio(M)\n\n0.9925308f0\n\n\n\ncontributions_of_components = (principalvars(M) ./ tvar(M) * 100)\n\n7-element Vector{Float32}:\n 43.02717\n 37.324966\n 15.405006\n  1.8183522\n  0.8683387\n  0.46416324\n  0.34509116\n\n\n\n# grab a few consecutive windows of data, encode and plot the data, the latent space and the principal components\ncomponents = Matrix{Float32}[]\nencoded = Matrix{Float32}[]\n\nstart_index = 101\n\nd = hcat(data[start_index],data[start_index+window_size], data[start_index+(window_size*2)], data[start_index+(window_size*3)])\nfor i ∈ 0:239\n    t = data[start_index + i]\n    encoded_t = new_model[1](Flux.unsqueeze(t',3))\n    push!(encoded, encoded_t)\n    push!(components,MultivariateStats.transform(M, encoded_t))\nend\n\n\ncreate_gif_of_data_latent_space_and_reduced(d, encoded,components, \"Principal components\")\n\n┌ Info: Saved animation to \n│   fn = /notebooks/anim_fps30.gif\n└ @ Plots /opt/julia/packages/Plots/LI4FE/src/animation.jl:114\n\n\n\n\n\n\n\nSince our method is windowed, a particular encoded representation and a particular set of PCA values actually represents a full window (60 frames / 2 seconds). In order to make the above, I calculated the encoding and the PCA projection for each window, using a step size of one (as was used in the training). So in fact the above is somewhat out of sync, since the encoded and PCA projections are 1 second in front of the raw data, and in fact referencing some data that isn’t visable in the Input Swarm visualisation.\n\n\n\n\nThe implementation of ISOMAP found in ManifoldLearning.jl is unusual for manifold learning algorithms because we can transform out of sample data into an existing model.\n\nM = fit(Isomap, (vcat(train_encoded...) |> f64)')\n\nIsomap{BruteForce}(indim = 100, outdim = 2, neighbors = 12)\n\n\n\nY = ManifoldLearning.transform(M)\n\n2×1749 Matrix{Float64}:\n 35.7851    -186.37      206.575     …  -60.2106    -72.1667     -37.1274\n -0.134194     0.145393    0.175012      -0.123771   -0.0541171   -0.130726\n\n\n\nPlots.scatter(T[1,:],T[2,:];label=\"\", title=\"2D manifold of encoded space by ISOMAP\")\n\n\n\n\n\n# Grab some data form the encoded test set\ne = rand(test_encoded)' |> f64;\n\n\n# Transform the out-of-sample test data into the existing ISOMAP model\n@time new_data = predict(M,e);\n\n  0.048351 seconds (15.30 k allocations: 25.509 MiB, 66.18% gc time)\n\n\n\n# Overlay the new data onto the existing 2D model\nPlots.scatter!([new_data[1]], [new_data[2]], label=\"new data\")\n\n\n\n\nI’m interested in reducing to more than 2 dimensions so lets try with 7 in line with the PCA example above\n\nM7 = fit(Isomap, (vcat(train_encoded...) |> f64)';maxoutdim=7)\n\nIsomap{BruteForce}(indim = 100, outdim = 7, neighbors = 12)\n\n\n\nisomap = Matrix{Float32}[]\nencoded = Matrix{Float32}[]\n\nstart_index = 101\n\nd = hcat(data[start_index],data[start_index+window_size], data[start_index+(window_size*2)], data[start_index+(window_size*3)])\nfor i ∈ 0:239\n    t = data[start_index + i]\n    encoded_t = new_model[1](Flux.unsqueeze(t',3)) |> f64\n    push!(encoded, encoded_t)\n    push!(isomap,predict(M7, encoded_t))\nend\n\n\ncreate_gif_of_data_latent_space_and_reduced(d, encoded,components, \"ISOMAP representation\")\n\n┌ Info: Saved animation to \n│   fn = /notebooks/anim_fps30.gif\n└ @ Plots /opt/julia/packages/Plots/LI4FE/src/animation.jl:114\n\n\n\n\n\n\n\n\n\n\n# use UMAP to do additional encoding on the latent space\n# unfortunatly there is no easy way to do out-of-sample transformation\n\nres_jl = umap(vcat(train_encoded...); n_neighbors=5, min_dist=0.001, n_epochs=100)\nPlots.scatter(res_jl[1,:], res_jl[2,:], title=\"UAMP encoding\", marker=(2, 2, :auto), label=\"\")\n\n\n\n\nInteresting to see some clusters emerge in the 2D representation. Could this be used for behaviour classification? Without out-of-sample data prediction, it’s not going to be useful in the context I want to use it."
  },
  {
    "objectID": "posts/2022-01-14-dcae-part-1.html",
    "href": "posts/2022-01-14-dcae-part-1.html",
    "title": "Deep Convolutional Auto Encoders - Part 1",
    "section": "",
    "text": "Introduction\nMy aim is to try and encode a simulated swarm into a lower dimentional representation so I can identify interesting parameters with which to map into a sound generating system.\nMy initial research into reducing complex time-series data lead me to a paper by Ali et al\nThey use convolutional layers in an auto encoder to do dimension reduction on various time series data.\nThis seemed like a good place to start, so initially I am just documenting my attempt to recreate their research, using a single agents coordinates in 3d space to match the shape of their model.\n\n\nDetails\nTheir approach uses 1D Convolution and the data split into interleaved sliding windows of data:\n\n\n\nThey used a 60 frame window and 3 readings from accelerometers resulting in a 60x3 shape input image.\nAs a test of their approach I used a single agents movement data to match the original model specifications.\nData is captured from a separate flocking simulation (source available on my github soon) of 300 agents over approximately 30 seconds.\n\nusing Flux, CSV, DataFrames, MLDataPattern, CUDA, Plots, WebIO;\nplotly()\n\nLoadError: ArgumentError: Package Flux not found in current path:\n- Run `import Pkg; Pkg.add(\"Flux\")` to install the Flux package.\n\n\n\n# select only the first 3 columns i.e. the x,y,z of a single agent\ndf = DataFrame(CSV.File(\"$(datadir())/exp_raw/data.csv\"; types=Float32))\ndf = df[:,1:3]\n\nplot(df[:,1],df[:,2], df[:,3])\n\n    \n    \n\n\nWe can treat this 3D data as 3 separate 1D data points, matching the original TimeCluster data shape\n\nplot(df[:,1], label=\"x\")\nplot!(df[:,2], label=\"y\")\nplot!(df[:,3], label=\"z\")\n\n    \n    \n\n\n\n\nData Noramlisation and preperation\nNormalise the data into the range [0, 1] as per the paper. We then create a sliding window using the defaults from the paper where stride = 1 and window_size = 60\n\nfunction normalise(M) \n    min = minimum(minimum(eachcol(M)))\n    max = maximum(maximum(eachcol(M)))\n    return (M .- min) ./ (max - min)\nend\n\nnormalised = Array(df) |> normalise\n\nwindow_size = 60\n\ndata = slidingwindow(normalised',window_size,stride=1);\n\n\n\nDefine the encoder and decoder\nWe create the auto-encoder network as per the paper\n\nfunction create_ae()\n    # Define the encoder and decoder networks \n  encoder = Chain(\n  # 60x3xb\n    Conv((10,), 3 => 64, relu; pad = SamePad()),\n    MaxPool((2,)),\n    # 30x64xb\n    Conv((5,), 64 => 32, relu; pad = SamePad()),\n    MaxPool((2,)),\n    # 15x32xb\n    Conv((5,), 32 => 12, relu; pad = SamePad()),\n    MaxPool((3,)),\n    # 5x12xb\n    Flux.flatten,\n    Dense(window_size,window_size)\n    )\n  decoder = Chain(\n    # input 60\n    (x -> reshape(x, (floor(Int, (window_size / 12)),12,:))),\n    # 5x12xb\n    ConvTranspose((5,), 12 => 32, relu; pad = SamePad()),\n    Upsample((3,)),\n    # 15x32xb\n    ConvTranspose((5,), 32 => 64, relu; pad = SamePad()),\n    Upsample((2,)),\n    # 30x64xb\n    ConvTranspose((10,), 64 => 3, relu; pad = SamePad()),\n    Upsample((2,)),\n    # 60x3xb\n  )\n  return Chain(encoder, decoder)\n end\n\ncreate_ae (generic function with 1 method)\n\n\n\n\nTraining\nIn keeping with the paper we use the Mean Square Error loss function and the ADAM optimiser\n\nfunction train_model!(model, data, opt; epochs=20, bs=16, dev=Flux.gpu)\n    model = model |> dev\n    ps = params(model)\n    t = shuffleobs(data)\n    local l\n    losses = Vector{Float64}()\n    for e in 1:epochs\n        for x in eachbatch(t, size=bs)\n            # bs[(3, 60)]\n            x  = cat(x..., dims=3)\n            # bs x 3 x 60\n            x  = permutedims(x, [2,1,3])\n            # 60 x 3 x bs\n            gs = gradient(ps) do\n                l = loss(model(x),x)\n            end\n            Flux.update!(opt, ps, gs)\n        end\n        l = round(l;digits=6)\n        push!(losses, l)\n        println(\"Epoch $e/$epochs - train loss: $l\")\n    end\n    model = model |> cpu;\n    losses\n end\n \n\ntrain_model! (generic function with 1 method)\n\n\n\nloss(x,y) = Flux.Losses.mse(x, y)\nopt       = Flux.Optimise.ADAM(0.00005)\nepochs    = 100\n\nloss (generic function with 1 method)\n\n\n\n#collapse_output    \nmodel     = create_ae()\nlosses_01 = train_model!(model, data, opt; epochs=epochs);\n\nEpoch 1/100 - train loss: 0.018689\nEpoch 2/100 - train loss: 0.004161\nEpoch 3/100 - train loss: 0.002182\nEpoch 4/100 - train loss: 0.001568\nEpoch 5/100 - train loss: 0.001264\nEpoch 6/100 - train loss: 0.001071\nEpoch 7/100 - train loss: 0.000899\nEpoch 8/100 - train loss: 0.000703\nEpoch 9/100 - train loss: 0.0005\nEpoch 10/100 - train loss: 0.000352\nEpoch 11/100 - train loss: 0.00028\nEpoch 12/100 - train loss: 0.000236\nEpoch 13/100 - train loss: 0.000202\nEpoch 14/100 - train loss: 0.000178\nEpoch 15/100 - train loss: 0.000159\nEpoch 16/100 - train loss: 0.000142\nEpoch 17/100 - train loss: 0.000127\nEpoch 18/100 - train loss: 0.000113\nEpoch 19/100 - train loss: 0.000101\nEpoch 20/100 - train loss: 9.2e-5\nEpoch 21/100 - train loss: 8.3e-5\nEpoch 22/100 - train loss: 7.5e-5\nEpoch 23/100 - train loss: 6.8e-5\nEpoch 24/100 - train loss: 6.3e-5\nEpoch 25/100 - train loss: 5.7e-5\nEpoch 26/100 - train loss: 5.2e-5\nEpoch 27/100 - train loss: 4.8e-5\nEpoch 28/100 - train loss: 4.4e-5\nEpoch 29/100 - train loss: 4.1e-5\nEpoch 30/100 - train loss: 3.8e-5\nEpoch 31/100 - train loss: 3.6e-5\nEpoch 32/100 - train loss: 3.4e-5\nEpoch 33/100 - train loss: 3.2e-5\nEpoch 34/100 - train loss: 3.0e-5\nEpoch 35/100 - train loss: 2.9e-5\nEpoch 36/100 - train loss: 2.7e-5\nEpoch 37/100 - train loss: 2.6e-5\nEpoch 38/100 - train loss: 2.4e-5\nEpoch 39/100 - train loss: 2.3e-5\nEpoch 40/100 - train loss: 2.3e-5\nEpoch 41/100 - train loss: 2.2e-5\nEpoch 42/100 - train loss: 2.1e-5\nEpoch 43/100 - train loss: 2.1e-5\nEpoch 44/100 - train loss: 2.1e-5\nEpoch 45/100 - train loss: 2.1e-5\nEpoch 46/100 - train loss: 2.0e-5\nEpoch 47/100 - train loss: 2.0e-5\nEpoch 48/100 - train loss: 2.0e-5\nEpoch 49/100 - train loss: 1.9e-5\nEpoch 50/100 - train loss: 1.9e-5\nEpoch 51/100 - train loss: 1.9e-5\nEpoch 52/100 - train loss: 1.9e-5\nEpoch 53/100 - train loss: 1.9e-5\nEpoch 54/100 - train loss: 1.9e-5\nEpoch 55/100 - train loss: 1.9e-5\nEpoch 56/100 - train loss: 1.8e-5\nEpoch 57/100 - train loss: 1.8e-5\nEpoch 58/100 - train loss: 1.7e-5\nEpoch 59/100 - train loss: 1.6e-5\nEpoch 60/100 - train loss: 1.5e-5\nEpoch 61/100 - train loss: 1.5e-5\nEpoch 62/100 - train loss: 1.4e-5\nEpoch 63/100 - train loss: 1.3e-5\nEpoch 64/100 - train loss: 1.3e-5\nEpoch 65/100 - train loss: 1.3e-5\nEpoch 66/100 - train loss: 1.3e-5\nEpoch 67/100 - train loss: 1.2e-5\nEpoch 68/100 - train loss: 1.2e-5\nEpoch 69/100 - train loss: 1.2e-5\nEpoch 70/100 - train loss: 1.2e-5\nEpoch 71/100 - train loss: 1.2e-5\nEpoch 72/100 - train loss: 1.2e-5\nEpoch 73/100 - train loss: 1.1e-5\nEpoch 74/100 - train loss: 1.1e-5\nEpoch 75/100 - train loss: 1.1e-5\nEpoch 76/100 - train loss: 1.1e-5\nEpoch 77/100 - train loss: 1.1e-5\nEpoch 78/100 - train loss: 1.0e-5\nEpoch 79/100 - train loss: 1.0e-5\nEpoch 80/100 - train loss: 1.0e-5\nEpoch 81/100 - train loss: 1.0e-5\nEpoch 82/100 - train loss: 1.0e-5\nEpoch 83/100 - train loss: 1.0e-5\nEpoch 84/100 - train loss: 1.0e-5\nEpoch 85/100 - train loss: 1.0e-5\nEpoch 86/100 - train loss: 1.0e-5\nEpoch 87/100 - train loss: 1.0e-5\nEpoch 88/100 - train loss: 1.0e-5\nEpoch 89/100 - train loss: 1.0e-5\nEpoch 90/100 - train loss: 1.0e-5\nEpoch 91/100 - train loss: 1.0e-5\nEpoch 92/100 - train loss: 1.0e-5\nEpoch 93/100 - train loss: 1.0e-5\nEpoch 94/100 - train loss: 1.0e-5\nEpoch 95/100 - train loss: 1.0e-5\nEpoch 96/100 - train loss: 1.0e-5\nEpoch 97/100 - train loss: 1.0e-5\nEpoch 98/100 - train loss: 1.0e-5\nEpoch 99/100 - train loss: 1.0e-5\nEpoch 100/100 - train loss: 1.0e-5\n\n\n\nplot(losses_01, label=\"\")\nxlabel!(\"Epochs\")\nylabel!(\"Mean Squared Error\")\n\n    \n    \n\n\nLets see how well it’s able to reconstruct a random segment of the data\n\n# Plot the input data in blue\ninput = rand(data)'\nplot(input[:,1],input[:, 2], input[:,3], label=\"original\")\n\n# Plot the reconstructed data in red\noutput = model(Flux.unsqueeze(input, 3))\nplot!(output[:,1], output[:, 2], output[:,3], label=\"reconstructed\")\n\n┌ Info: loss:\n│   loss(input, output) = 6.2904514e-6\n└ @ Main In[72]:6\n\n\n    \n    \n\n\n\n# Just for interest what is the loss of the above reconstruction\n@info \"loss:\" loss(input, output)\n\n┌ Info: loss:\n│   loss(input, output) = 6.2904514e-6\n└ @ Main In[74]:2\n\n\n\n\nConclusion\nThis approach clearly is able to reconstruct the input data from a low dimensional representation. Before we go further (and use clustering on the latent space to identify parameters), lets try and scale this up to a 300 agent swarm and see what changes we need to make."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "recent posts",
    "section": "",
    "text": "Sharing matrix data between OpenFrameworks and Julia\n\n\n\n\n\n\n\nJulia\n\n\nOpenFrameworks\n\n\nMemory\n\n\nPOSIX\n\n\n\n\nUsing the POSIX shared memory API to allow shared access to swarm data\n\n\n\n\n\n\nMar 15, 2022\n\n\nMax Worgan\n\n\n\n\n\n\n\n\nDimension reduction of the latent space\n\n\n\n\n\n\n\ndimension reduction\n\n\nauto-encoder\n\n\nswarm\n\n\n\n\nFrom a latent space to musical parameters\n\n\n\n\n\n\nFeb 28, 2022\n\n\nMax Worgan\n\n\n\n\n\n\n\n\nDeep Convolutional Auto Encoders - Part 2\n\n\n\n\n\n\n\ndimension reduction\n\n\nauto-encoder\n\n\nswarm\n\n\n\n\nAdaptation to encode entire swarm\n\n\n\n\n\n\nFeb 6, 2022\n\n\nMax Worgan\n\n\n\n\n\n\n\n\nDeep Convolutional Auto Encoders - Part 1\n\n\n\n\n\n\n\ndimension reduction\n\n\nauto-encoder\n\n\nswarm\n\n\n\n\nReimplementing DCAE to encode agent swarm flight\n\n\n\n\n\n\nJan 14, 2022\n\n\nMax Worgan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]